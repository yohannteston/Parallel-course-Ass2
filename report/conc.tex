\chapter{Conclusion}

The speedup depends both on the number of threads involved and the size of the array to sort.\\ 

If there is not enough data to justify the use of threads and so the overheads for creating and joining them, the speedup will be bad, often less than 1. On the contrary, bigger amounts of data get good results. The number of threads has to be taken into account as well. The more threads we use, the more data we need to explain their use and keep a good speedup. The test we did with an array of $10000$ elements is a good example. We can see that this is enough data for 2 threads but then the speedup keeps decreasing and ends up less than 1. Another example is the array of $100000$ that gets good results with up to $2^5$ (32) threads involved but sees those results plummet as more threads are involved. It is so because creating threads, scheduling and joining them has an overhead that becomes too important compared to the computations if the amount of data is not big enough.\\

The quick-sort's results are also closely related to the choice of the pivot. Choosing a bad pivot can make the algorithm complexity degenerate to $\Theta(n^2)$ instead of $\Theta(n\log n)$. This is especially true in a parallel formulation of this algorithm where choosing a bad pivot can lead to a bad data partitioning with some threads having a lot more data to treat than the others. The worst case would be that a thread becomes idle while the others are working with a big amount of data. Such cases lead to bad performances. Unfortunately, the pivot selection is a difficult problem.\\

Since our algorithm creates a new thread each time the data is partitioned and allocates it one of these parts, the algorithm's performances can greatly be affected by a bad pivot choice. Indeed, it is possible to have two segments, one consisting of all the data except one and another with a unique value. In that case, a new thread will be created just to "sort" this unique value while the other one will have a lot of work to do. Obviously, our algorithm's performances would be bad in that case. \\

An improvement that can be done to our code is then to implement a better pivot selection algorithm. We chose to implement the median of three algorithm that chooses as the pivot the median between the bounds values of an interval and its middle value. It gives good enough results but there exists more efficient algorithms. One way could be to calculate the median of the values to sort and use it as the pivot. The problem is that finding the median is a costly problem as well. The median can also be approximed using some hypothesis on the data (like an uniform distribution). Ensuring the choice of a good pivot is therefore a good way to get good results, but one has to be careful concerning the time taken by the pivot selection as spending too much time doing it would result in poor performances.\\

To conclude, we can say that a parallel quicksort must be justified by the amount of data it has to sort and/or the number of threads used. If the amount of data is too small, the overhead created by using threads will be too important and the performances will drop. If too many threads are involved for a given amount of data, the same thing will happen. The pivot selection can also affect seriously the results. Therefore, good results with this algorithm are obtained through a big enough amount of data and a realistic number of threads compared to the latter.\\
